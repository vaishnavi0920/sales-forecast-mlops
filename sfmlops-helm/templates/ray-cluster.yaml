## from https://github.com/ray-project/kuberay/blob/v1.0.0/ray-operator/config/samples/ray-cluster.complete.yaml
# The resource requests and limits in this config are too small for production!
# For examples with more realistic resource configuration, see
# ray-cluster.complete.large.yaml and
# ray-cluster.autoscaler.large.yaml.
apiVersion: ray.io/v1
kind: RayCluster
metadata:
  labels:
    controller-tools.k8s.io: "1.0"
    # A unique identifier for the head node and workers of this cluster.
  name: raycluster-complete
spec:
  rayVersion: '2.9.3'
  # Ray head pod configuration
  headGroupSpec:
    # Kubernetes Service Type. This is an optional field, and the default value is ClusterIP.
    # Refer to https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types.
    serviceType: LoadBalancer
    # The `rayStartParams` are used to configure the `ray start` command.
    # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
    # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
    rayStartParams:
      dashboard-host: '0.0.0.0'
    headService:
      metadata:
        name: ray-head-service
    # pod template
    template:
      metadata:
        # Custom labels. NOTE: To avoid conflicts with KubeRay operator, do not define custom labels start with `raycluster`.
        # Refer to https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        labels: {}
      spec:
        # this is a just-work solution for configuring permission for pvc,
        # but introduces a security risk
        # in real production, please setup user and group id properly in each image
        # and assign enough permissions for that
        initContainers:
        - name: change-ownership-container
          image: busybox
          command: ["/bin/chmod", "-R", "777", {{ .Values.mlflowArtifactRoot }}] # Adjust mount path if needed
          securityContext:
            runAsUser: 0
            privileged: true
          volumeMounts:
            - name: mlflow-data
              mountPath: {{ .Values.mlflowArtifactRoot }}
        containers:
        - name: ray-head
          image: {{ .Values.containerMainImage.ray }}
          imagePullPolicy: Always
          ports:
          - containerPort: 6379
            name: gcs
          - containerPort: 8265
            name: dashboard
          - containerPort: 10001
            name: client
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","ray stop"]
          envFrom:
            - configMapRef:
                name: global-configmap
          env:
            - name: MLFLOW_TRACKING_URI
              value: "http://mlflow-service:5050"
            - name: DB_CONNECTION_URL
              value: "postgresql://spark_user:SuperSecurePwdHere@postgres-service:5432/spark_pg_db"
          volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
            - name: mlflow-data
              mountPath: {{ .Values.mlflowArtifactRoot }}
          # The resource requests and limits in this config are too small for production!
          # For an example with more realistic resource configuration, see
          # ray-cluster.autoscaler.large.yaml.
          # It is better to use a few large Ray pod than many small ones.
          # For production, it is ideal to size each Ray pod to take up the
          # entire Kubernetes node on which it is scheduled.
          resources:
            limits:
              cpu: "1"
              memory: "2G"
            requests:
              # For production use-cases, we recommend specifying integer CPU reqests and limits.
              # We also recommend setting requests equal to limits for both CPU and memory.
              # For this example, we use a 500m CPU request to accomodate resource-constrained local
              # Kubernetes testing environments such as KinD and minikube.
              cpu: "500m"
              memory: "2G"
        volumes:
          - name: ray-logs
            emptyDir: {}
          - name: mlflow-data
            persistentVolumeClaim:
              claimName: {{ .Values.mlflowPvcName }}
  workerGroupSpecs:
  # the pod replicas in this group typed worker
  - replicas: 1
    minReplicas: 1
    maxReplicas: 10
    # logical group name, for this called small-group, also can be functional
    groupName: small-group
    # If worker pods need to be added, we can increment the replicas.
    # If worker pods need to be removed, we decrement the replicas, and populate the workersToDelete list.
    # The operator will remove pods from the list until the desired number of replicas is satisfied.
    # If the difference between the current replica count and the desired replicas is greater than the
    # number of entries in workersToDelete, random worker pods will be deleted.
    #scaleStrategy:
    #  workersToDelete:
    #  - raycluster-complete-worker-small-group-bdtwh
    #  - raycluster-complete-worker-small-group-hv457
    #  - raycluster-complete-worker-small-group-k8tj7
    # The `rayStartParams` are used to configure the `ray start` command.
    # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
    # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
    rayStartParams: {}
    #pod template
    template:
      spec:
        initContainers:
        - name: change-ownership-container
          image: busybox
          command: ["/bin/chmod", "-R", "777", {{ .Values.mlflowArtifactRoot }}] # Adjust mount path if needed
          securityContext:
            runAsUser: 0
            privileged: true
          volumeMounts:
            - name: mlflow-data
              mountPath: {{ .Values.mlflowArtifactRoot }}
        containers:
        - name: ray-worker
          image: {{ .Values.containerMainImage.ray }}
          imagePullPolicy: Always
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh","-c","ray stop"]
          envFrom:
            - configMapRef:
                name: global-configmap
          env:
            - name: MLFLOW_TRACKING_URI
              value: "http://mlflow-service:5050"
            - name: DB_CONNECTION_URL
              value: "postgresql://spark_user:SuperSecurePwdHere@postgres-service:5432/spark_pg_db"
          # use volumeMounts.Optional.
          # Refer to https://kubernetes.io/docs/concepts/storage/volumes/
          volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
            - name: mlflow-data
              mountPath: {{ .Values.mlflowArtifactRoot }}
          # The resource requests and limits in this config are too small for production!
          # For an example with more realistic resource configuration, see
          # ray-cluster.autoscaler.large.yaml.
          # It is better to use a few large Ray pod than many small ones.
          # For production, it is ideal to size each Ray pod to take up the
          # entire Kubernetes node on which it is scheduled.
          resources:
            limits:
              cpu: "1"
              memory: "1G"
            # For production use-cases, we recommend specifying integer CPU reqests and limits.
            # We also recommend setting requests equal to limits for both CPU and memory.
            # For this example, we use a 500m CPU request to accomodate resource-constrained local
            # Kubernetes testing environments such as KinD and minikube.
            requests:
              # For production use-cases, we recommend specifying integer CPU reqests and limits.
              # We also recommend setting requests equal to limits for both CPU and memory.
              # For this example, we use a 500m CPU request to accomodate resource-constrained local
              # Kubernetes testing environments such as KinD and minikube.
              cpu: "500m"
              # For production use-cases, we recommend allocating at least 8Gb memory for each Ray container.
              memory: "1G"
        # use volumes
        # Refer to https://kubernetes.io/docs/concepts/storage/volumes/
        volumes:
          - name: ray-logs
            emptyDir: {}
          - name: mlflow-data
            persistentVolumeClaim:
              claimName: {{ .Values.mlflowPvcName }}
######################status#################################
